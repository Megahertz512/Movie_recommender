{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The geckodriver version (0.33.0) detected in PATH at e:\\Projects\\Python\\Movie_recommender\\geckodriver.exe might not be compatible with the detected firefox version (125.0.2.8875); currently, geckodriver 0.34.0 is recommended for firefox 125.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Initialize Selenium WebDriver for Firefox\n",
    "firefox_options = Options()\n",
    "# firefox_options.add_argument(\"--headless\")  # Run headless Firefox to avoid opening a browser window\n",
    "# service = FirefoxService(GeckoDriverManager().install())\n",
    "driver = webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "# Open the web page\n",
    "url = \"https://www.imdb.com/chart/top/\"  # Replace with the URL of the page you want to scrape\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely (this time can be adjusted)\n",
    "driver.implicitly_wait(10)  # seconds\n",
    "\n",
    "#scroll to the bottom of the page\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Get the page source after JavaScript has executed\n",
    "html = driver.page_source\n",
    "\n",
    "# Pass the HTML to BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Now you can use BeautifulSoup to parse the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<a class=\"ipc-title-link-wrapper\" href=\"/title/tt0111161/?ref_=chttp_t_1\" tabindex=\"0\"><h3 class=\"ipc-title__text\">1. The Shawshank Redemption</h3></a>',\n",
       " '<a class=\"ipc-title-link-wrapper\" href=\"/title/tt0068646/?ref_=chttp_t_2\" tabindex=\"0\"><h3 class=\"ipc-title__text\">2. The Godfather</h3></a>',\n",
       " '<a class=\"ipc-title-link-wrapper\" href=\"/title/tt0468569/?ref_=chttp_t_3\" tabindex=\"0\"><h3 class=\"ipc-title__text\">3. The Dark Knight</h3></a>',\n",
       " '<a class=\"ipc-title-link-wrapper\" href=\"/title/tt0071562/?ref_=chttp_t_4\" tabindex=\"0\"><h3 class=\"ipc-title__text\">4. The Godfather Part II</h3></a>',\n",
       " '<a class=\"ipc-title-link-wrapper\" href=\"/title/tt0050083/?ref_=chttp_t_5\" tabindex=\"0\"><h3 class=\"ipc-title__text\">5. 12 Angry Men</h3></a>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracts links from first web\n",
    "scraped_movies = soup.find_all('a' , class_ = 'ipc-title-link-wrapper')\n",
    "list_scraped_movie = [str(i) for i in scraped_movies]\n",
    "list_scraped_movie[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We Use regex to find links in strings\n",
    "import re\n",
    "\n",
    "def extract_url(html_string):\n",
    "    match = re.search( r'href=\"([^\"]+?)\\?', html_string)\n",
    "    return match.group(1)\n",
    "\n",
    "\n",
    "extracted_urls = [extract_url(movie) for movie in list_scraped_movie]\n",
    "\n",
    "extracted_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_scraped_movie) #Only first 250 are usefuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_of_movies = [i.get_text() for i in scraped_movies][:250]\n",
    "Name_of_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Create an example for first Movie ###\n",
    "###(The Shawshank Redemption)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE at first run it for an examole to see whats happen\n",
    "\n",
    "import requests\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
    "url_for_movie = 'https://www.imdb.com/' + extracted_urls[0] + 'plotsummary/?ref_=tt_stry_pl'\n",
    "page_for_movie = requests.get(url_for_movie , headers = HEADERS)\n",
    "content_for_movie =page_for_movie.text\n",
    "soup_for_movie = BeautifulSoup(content_for_movie , 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_for_movie.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = soup_for_movie.find_all('div' , class_ = 'ipc-html-content ipc-html-content--base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main Web scraping begins!!\n",
    "\n",
    "data = []\n",
    "for i in range(250):\n",
    "    url_for_movie = 'https://www.imdb.com/' + extracted_urls[i] + 'plotsummary/?ref_=tt_stry_pl'\n",
    "    page_for_movie = requests.get(url_for_movie , headers = HEADERS)\n",
    "    content_for_movie =page_for_movie.text\n",
    "    soup_for_movie = BeautifulSoup(content_for_movie , 'html.parser')\n",
    "\n",
    "    m1 = soup_for_movie.find_all('div' , class_ = 'ipc-html-content ipc-html-content--base')\n",
    "    data.append(m1[1].get_text().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We store it to a dataframe for saving in a excel file\n",
    "\n",
    "df = pd.DataFrame(data = {'Name_of_movies': Name_of_movies , 'data' : data})\n",
    "# df = pd.read_excel('imdb.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name_of_movies</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. The Shawshank Redemption</td>\n",
       "      <td>chronicles the experiences of a formerly succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. The Godfather</td>\n",
       "      <td>the godfather \"don\" vito corleone is the head ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. The Dark Knight</td>\n",
       "      <td>set within a year after the events of batman b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. The Godfather Part II</td>\n",
       "      <td>the continuing saga of the corleone crime fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. 12 Angry Men</td>\n",
       "      <td>the defense and the prosecution have rested, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name_of_movies  \\\n",
       "0  1. The Shawshank Redemption   \n",
       "1             2. The Godfather   \n",
       "2           3. The Dark Knight   \n",
       "3     4. The Godfather Part II   \n",
       "4              5. 12 Angry Men   \n",
       "\n",
       "                                                data  \n",
       "0  chronicles the experiences of a formerly succe...  \n",
       "1  the godfather \"don\" vito corleone is the head ...  \n",
       "2  set within a year after the events of batman b...  \n",
       "3  the continuing saga of the corleone crime fami...  \n",
       "4  the defense and the prosecution have rested, a...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Save that information on an excel file  for  backup ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('imdb.xlsx' , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing data (removing stop words , ...) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = 'a, an, and, are, as, at, be, but, by, for, if, in, into, is, it, no, not, of, on, or, such, that, the, their, then, there, these, they, this, to, was, will and with.'.split(', ')\n",
    "special_charcters = ['`','~','!','@','#','$','%','^','&','*','(',')','_','-','+','=','{','[','}','}','|','\\\\', ':' , ';','\"', \"'\",'<',',','>','.','?','//' ]\n",
    "number = [str(i) for i in range(9)]\n",
    "\n",
    "Idontwantthem = stopwords + special_charcters + number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it if you useing excel file that attach to it\n",
    "data = list(df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We try to clean data from stopwords, special charcters and numbers\n",
    "cleaned_data = []\n",
    "for string in data:\n",
    "    for unwanted in Idontwantthem:\n",
    "        string = string.replace(unwanted , '')\n",
    "    cleaned_data.append(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TF_idf ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_words = set()\n",
    "for j in cleaned_data:\n",
    "   All_words = All_words.union({word for word in j})\n",
    "All_words = list(All_words)\n",
    "len(All_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = np.zeros(shape = (len(cleaned_data) , len(All_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_tf(word, document):\n",
    "\n",
    "  return document.count(word) / len(document)\n",
    "\n",
    "\n",
    "def calculate_idf(word, corpus):\n",
    "\n",
    "  document_count = len(corpus)\n",
    "  containing_documents = sum(1 for document in corpus if word in document)\n",
    "  return math.log(document_count / (containing_documents + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = {}\n",
    "for word in All_words:\n",
    "    idfs[word] = calculate_idf(word , cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for i in range(len(cleaned_data)):\n",
    "    for j in range(len(All_words)):\n",
    "        tf_idf[i][j] = idfs[All_words[j]] * calculate_tf(All_words[j] , cleaned_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with cosine ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a , b):\n",
    "    return np.dot(a , b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07128307])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.random.rand(1,len(All_words)) , tf_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_with_cosine(a , tf_idf , Name_of_movies , k):\n",
    "    distances = []\n",
    "    for i in range(len(cleaned_data)):\n",
    "        distances.append((cosine_similarity(a , tf_idf[i]) , i) )\n",
    "    sorted_distance = sorted(distances , key = lambda x: -x[0])\n",
    "\n",
    "    return [ Name_of_movies[j] for _,j in sorted_distance[:k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2. The Godfather', '4. The Godfather Part II', '196. The Deer Hunter']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_with_cosine(tf_idf[1] , tf_idf , Name_of_movies , 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
